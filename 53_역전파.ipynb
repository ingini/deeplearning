{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 빅데이터 활용 AI 설계\n",
    "# 역전파 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기울기 계산 일반화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ u_i = \\sum_j (w_j \\cdot x_{ij}) + b \\\\\n",
    "{\\partial Loss \\over \\partial w_j} = -{2 \\over N} \\sum_i (y_i - \\hat{y}_i) \\cdot f'(u_i) \\cdot  x_{ij} \\\\\n",
    "{\\partial Loss \\over \\partial b} = -{2 \\over N} \\sum_i (y_i - \\hat{y_i}) \\cdot f'(u_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='역전파.jpg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 역전파 알고리즘들\n",
    "- https://en.wikipedia.org/wiki/Stochastic_gradient_descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD (Stochastic Gradient Descent)\n",
    "- 'Stochastic' 은 랜덤하게 샘플을 뽑는다는 의미임\n",
    "- 기본적인 경사하강법을 샘플에 적용함\n",
    "- 추가적으로 모멘텀 적용 가능\n",
    "> $ W(t) = W(t-1) - \\eta \\cdot \\nabla_W Loss + \\alpha \\cdot \\Delta W(t-1) $\n",
    "\n",
    "<img src='https://tensorflowkorea.files.wordpress.com/2017/03/ec8aa4ed81aceba6b0ec83b7-2017-03-21-ec98a4ed9b84-3-22-52.png?w=625' />\n",
    "\n",
    "#### Adagrad\n",
    "- 각 가중치(w) 마다 학습률을 다르게 설정\n",
    "- 변화가 많았던 가중치는 적게, 변화가 적었던 가중치는 많게 학습률을 적용한다.\n",
    "> $ W(t) = W(t-1) - { \\eta \\over \\sqrt{\\sum \\nabla Loss} } \\cdot \\nabla_W Loss $\n",
    "\n",
    "#### RMSprop\n",
    "- Adagrad 의 단점을 개선한 알고리즘\n",
    "- 학습이 진행될수록 학습률이 너무 작아지는 경향을 보정<br>\n",
    "> <img src='https://wikimedia.org/api/rest_v1/media/math/render/svg/2964cc8dc82a134dd4f20e42094f56410b0d2d9c' />\n",
    "> <img src='https://wikimedia.org/api/rest_v1/media/math/render/svg/fc46ae8619e71130c6c8212eec31560cb4891c0a' />\n",
    "\n",
    "#### Adam\n",
    "- 모멘텀과 RMSprop 의 장점을 결합<br>\n",
    "> <img src='https://wikimedia.org/api/rest_v1/media/math/render/svg/e388b9155519b8769930b3764f4dadc20eb593b8' />\n",
    "> <img src='https://wikimedia.org/api/rest_v1/media/math/render/svg/034d5652b502094ab7f58f95a383e0ec41de5b77' />\n",
    "> <img src='https://wikimedia.org/api/rest_v1/media/math/render/svg/1625bff4ce904cc83c3cadad4bc1a2ff61422b02' />\n",
    "> <img src='https://wikimedia.org/api/rest_v1/media/math/render/svg/7c5ea1207fc3574a51d439f84370a989deffa871' />\n",
    "> <img src='https://wikimedia.org/api/rest_v1/media/math/render/svg/abcd4c729bac933249992e086fa1ba7807e1cd09' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://t1.daumcdn.net/cfile/tistory/99399C355AA816740F' />\n",
    "(출처: https://kolikim.tistory.com/47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, Adagrad, RMSprop, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SGD in module keras.optimizers:\n",
      "\n",
      "class SGD(Optimizer)\n",
      " |  Stochastic gradient descent optimizer.\n",
      " |  \n",
      " |  Includes support for momentum,\n",
      " |  learning rate decay, and Nesterov momentum.\n",
      " |  \n",
      " |  # Arguments\n",
      " |      lr: float >= 0. Learning rate.\n",
      " |      momentum: float >= 0. Parameter that accelerates SGD\n",
      " |          in the relevant direction and dampens oscillations.\n",
      " |      decay: float >= 0. Learning rate decay over each update.\n",
      " |      nesterov: boolean. Whether to apply Nesterov momentum.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SGD\n",
      " |      Optimizer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, lr=0.01, momentum=0.0, decay=0.0, nesterov=False, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |  \n",
      " |  get_updates(self, loss, params)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Optimizer:\n",
      " |  \n",
      " |  get_gradients(self, loss, params)\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current value of the weights of the optimizer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A list of numpy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the optimizer, from Numpy arrays.\n",
      " |      \n",
      " |      Should only be called after computing the gradients\n",
      " |      (otherwise the optimizer has no weights).\n",
      " |      \n",
      " |      # Arguments\n",
      " |          weights: a list of Numpy arrays. The number\n",
      " |              of arrays and their shape must match\n",
      " |              number of the dimensions of the weights\n",
      " |              of the optimizer (i.e. it should match the\n",
      " |              output of `get_weights`).\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: in case of incompatible weight shapes.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from Optimizer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Optimizer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(SGD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
