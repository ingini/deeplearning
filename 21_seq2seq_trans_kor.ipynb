{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 빅데이터 활용 AI 설계\n",
    "# Seq2seq Eng-Kor Translater\n",
    "- Neural Machine Translation using word level seq2seq model<br>(https://medium.com/@dev.elect.iitd/neural-machine-translation-using-word-level-seq2seq-model-47538cba8cd7)\n",
    "- 원본 소스 : https://github.com/devm2024/nmt_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "from string import digits\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영어-한국어 번역 파일 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('kor.txt', delimiter='\\t', names=['eng', 'kor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>kor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who?</td>\n",
       "      <td>누구?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hello!</td>\n",
       "      <td>안녕!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No way!</td>\n",
       "      <td>절대 아니야.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No way!</td>\n",
       "      <td>그럴리가!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Goodbye!</td>\n",
       "      <td>안녕!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        eng      kor\n",
       "0      Who?      누구?\n",
       "1    Hello!      안녕!\n",
       "2   No way!  절대 아니야.\n",
       "3   No way!    그럴리가!\n",
       "4  Goodbye!      안녕!"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>kor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>Tom was standing outside the window, listening...</td>\n",
       "      <td>톰은 메리와 존이 말하는 것을 들으며 창문 밖에 서 있었다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>Tom occasionally prods me to work out with him...</td>\n",
       "      <td>톰은 때때로 같이 운동하자고 졸라댔지만 나는 정말 운동에 관심이 없다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>I think I can speak French well enough to say ...</td>\n",
       "      <td>나는 내가 말하고 싶은 것은 거의 다 말할 수 있을 정도로 불어를 잘 한다고 생각한다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>It's not always possible to eat well when you'...</td>\n",
       "      <td>당신이 세계를 여행하는 동안, 항상 잘먹는 것이 가능하지는 않습니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>Make a good translation of the sentence that y...</td>\n",
       "      <td>당신이 번역할 문장에 충실하게 번역하십시오. 다른 언어로 번역된 문장의 영향을 받지...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   eng  \\\n",
       "904  Tom was standing outside the window, listening...   \n",
       "905  Tom occasionally prods me to work out with him...   \n",
       "906  I think I can speak French well enough to say ...   \n",
       "907  It's not always possible to eat well when you'...   \n",
       "908  Make a good translation of the sentence that y...   \n",
       "\n",
       "                                                   kor  \n",
       "904                  톰은 메리와 존이 말하는 것을 들으며 창문 밖에 서 있었다.  \n",
       "905            톰은 때때로 같이 운동하자고 졸라댔지만 나는 정말 운동에 관심이 없다.  \n",
       "906   나는 내가 말하고 싶은 것은 거의 다 말할 수 있을 정도로 불어를 잘 한다고 생각한다.  \n",
       "907             당신이 세계를 여행하는 동안, 항상 잘먹는 것이 가능하지는 않습니다.  \n",
       "908  당신이 번역할 문장에 충실하게 번역하십시오. 다른 언어로 번역된 문장의 영향을 받지...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(909, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 900개 문장만 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = df[:900].copy()\n",
    "lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>kor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>Tom gave Mary some advice on how to pass multi...</td>\n",
       "      <td>탐은 메리에게 선다형 시험에 통과하는 방법에 대해 조언했다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>Tom talks to his mother more often than he tal...</td>\n",
       "      <td>톰은 아버지보다 어머니와 대화를 더 자주 한다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>That would be like sawing off the branch that ...</td>\n",
       "      <td>그건 니가 앉아있는 가지를 쳐내는 꼴이야.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>How can anyone trust anything Tom says? He's a...</td>\n",
       "      <td>어떻게 누군가가 톰이 말하는 것을 믿을 수 있지? 그는 병적인 거짓말쟁이야.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>After driving for eight hours, Tom just couldn...</td>\n",
       "      <td>8시간 동안 운전을 한 후에 톰은 더이상 운전을 할 수 없었다.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   eng  \\\n",
       "895  Tom gave Mary some advice on how to pass multi...   \n",
       "896  Tom talks to his mother more often than he tal...   \n",
       "897  That would be like sawing off the branch that ...   \n",
       "898  How can anyone trust anything Tom says? He's a...   \n",
       "899  After driving for eight hours, Tom just couldn...   \n",
       "\n",
       "                                            kor  \n",
       "895           탐은 메리에게 선다형 시험에 통과하는 방법에 대해 조언했다.  \n",
       "896                  톰은 아버지보다 어머니와 대화를 더 자주 한다.  \n",
       "897                     그건 니가 앉아있는 가지를 쳐내는 꼴이야.  \n",
       "898  어떻게 누군가가 톰이 말하는 것을 믿을 수 있지? 그는 병적인 거짓말쟁이야.  \n",
       "899         8시간 동안 운전을 한 후에 톰은 더이상 운전을 할 수 없었다.  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.eng=lines.eng.apply(lambda x: x.lower())\n",
    "lines.kor=lines.kor.apply(lambda x: x.lower())\n",
    "\n",
    "lines.eng=lines.eng.apply(lambda x: re.sub(\"'\", '', x)).apply(lambda x: re.sub(\",\", ' COMMA', x))\n",
    "lines.kor=lines.kor.apply(lambda x: re.sub(\"'\", '', x)).apply(lambda x: re.sub(\",\", ' COMMA', x))\n",
    "\n",
    "exclude = set(string.punctuation) # '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "lines.eng=lines.eng.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "lines.kor=lines.kor.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "\n",
    "# 숫자를 지운다\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "lines.eng=lines.eng.apply(lambda x: x.translate(remove_digits))\n",
    "lines.kor=lines.kor.apply(lambda x: x.translate(remove_digits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어 문장에 태그 달기\n",
    "- START_ 와 _END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.kor = lines.kor.apply(lambda x : 'START_ '+ x + ' _END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>kor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>who</td>\n",
       "      <td>START_ 누구 _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hello</td>\n",
       "      <td>START_ 안녕 _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no way</td>\n",
       "      <td>START_ 절대 아니야 _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no way</td>\n",
       "      <td>START_ 그럴리가 _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>goodbye</td>\n",
       "      <td>START_ 안녕 _END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       eng                 kor\n",
       "0      who      START_ 누구 _END\n",
       "1    hello      START_ 안녕 _END\n",
       "2   no way  START_ 절대 아니야 _END\n",
       "3   no way    START_ 그럴리가 _END\n",
       "4  goodbye      START_ 안녕 _END"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 목록 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_words=set()\n",
    "for eng in lines.eng:\n",
    "    for word in eng.split():\n",
    "        eng_words.add(word)\n",
    "    \n",
    "kor_words=set()\n",
    "for kor in lines.kor:\n",
    "    for word in kor.split():\n",
    "        kor_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1138, 1990)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_words = sorted(list(eng_words))\n",
    "kor_words = sorted(list(kor_words))\n",
    "\n",
    "len(eng_words), len(kor_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COMMA',\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'abroad',\n",
       " 'absolutely',\n",
       " 'accept',\n",
       " 'accurate',\n",
       " 'actions',\n",
       " 'active']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['COMMA',\n",
       " 'START_',\n",
       " '_END',\n",
       " 'a와',\n",
       " 'birthday',\n",
       " 'b의',\n",
       " 'happy',\n",
       " 'mary가',\n",
       " 'tom과',\n",
       " 'tom은']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(eng_words[:10], kor_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 사전 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_w2i = { word: i for i, word in enumerate(eng_words) }\n",
    "kor_w2i = { word: i for i, word in enumerate(kor_words) }\n",
    "\n",
    "eng_i2w = { i: word for word, i in eng_w2i.items() }\n",
    "kor_i2w = { i: word for word, i in kor_w2i.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1138, 1990)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eng_i2w), len(kor_i2w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최대 문장 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 15)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_sen_max = max([len(sen.split()) for sen in lines.eng])\n",
    "kor_sen_max = max([len(sen.split()) for sen in lines.kor])\n",
    "\n",
    "eng_sen_max, kor_sen_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 입력/출력 데이터 생성 (어레이)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros([len(lines), eng_sen_max])\n",
    "decoder_input_data = np.zeros([len(lines), kor_sen_max])\n",
    "decoder_output_data = np.zeros([len(lines), kor_sen_max, len(kor_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line, (eng,kor) in enumerate(zip(lines.eng, lines.kor)):\n",
    "    for t, word in enumerate(eng.split()):\n",
    "        encoder_input_data[line,t] = eng_w2i[word]\n",
    "        \n",
    "    for t, word in enumerate(kor.split()):\n",
    "        decoder_input_data[line,t] = kor_w2i[word]\n",
    "        if t>0:\n",
    "            decoder_output_data[line,t-1,kor_w2i[word]] = 1 # START_ 가 빠지고 하나씩 당겨짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 인코더  생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\py36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 50\n",
    "\n",
    "enc_inputs = Input((eng_sen_max,))\n",
    "enc_embed = Embedding(len(eng_words),embedding_size)(enc_inputs)\n",
    "\n",
    "enc_lstm = LSTM(50, return_state=True)\n",
    "enc_output, state_h, state_c = enc_lstm(enc_embed)\n",
    "\n",
    "enc_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 디코더 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_inputs = Input((kor_sen_max,))\n",
    "dec_embed = Embedding(len(kor_words), embedding_size)(dec_inputs)\n",
    "\n",
    "dec_lstm = LSTM(50, return_sequences=True)\n",
    "dec_outputs = dec_lstm(dec_embed, initial_state=enc_states)\n",
    "\n",
    "dec_outputs = Dense(len(kor_words), activation='softmax')(dec_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([enc_inputs, dec_inputs], dec_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\py36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LSTM in module keras.layers.recurrent:\n",
      "\n",
      "class LSTM(RNN)\n",
      " |  Long Short-Term Memory layer - Hochreiter 1997.\n",
      " |  \n",
      " |  # Arguments\n",
      " |      units: Positive integer, dimensionality of the output space.\n",
      " |      activation: Activation function to use\n",
      " |          (see [activations](../activations.md)).\n",
      " |          Default: hyperbolic tangent (`tanh`).\n",
      " |          If you pass `None`, no activation is applied\n",
      " |          (ie. \"linear\" activation: `a(x) = x`).\n",
      " |      recurrent_activation: Activation function to use\n",
      " |          for the recurrent step\n",
      " |          (see [activations](../activations.md)).\n",
      " |          Default: hard sigmoid (`hard_sigmoid`).\n",
      " |          If you pass `None`, no activation is applied\n",
      " |          (ie. \"linear\" activation: `a(x) = x`).\n",
      " |      use_bias: Boolean, whether the layer uses a bias vector.\n",
      " |      kernel_initializer: Initializer for the `kernel` weights matrix,\n",
      " |          used for the linear transformation of the inputs.\n",
      " |          (see [initializers](../initializers.md)).\n",
      " |      recurrent_initializer: Initializer for the `recurrent_kernel`\n",
      " |          weights matrix,\n",
      " |          used for the linear transformation of the recurrent state.\n",
      " |          (see [initializers](../initializers.md)).\n",
      " |      bias_initializer: Initializer for the bias vector\n",
      " |          (see [initializers](../initializers.md)).\n",
      " |      unit_forget_bias: Boolean.\n",
      " |          If True, add 1 to the bias of the forget gate at initialization.\n",
      " |          Setting it to true will also force `bias_initializer=\"zeros\"`.\n",
      " |          This is recommended in [Jozefowicz et al.]\n",
      " |          (http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf).\n",
      " |      kernel_regularizer: Regularizer function applied to\n",
      " |          the `kernel` weights matrix\n",
      " |          (see [regularizer](../regularizers.md)).\n",
      " |      recurrent_regularizer: Regularizer function applied to\n",
      " |          the `recurrent_kernel` weights matrix\n",
      " |          (see [regularizer](../regularizers.md)).\n",
      " |      bias_regularizer: Regularizer function applied to the bias vector\n",
      " |          (see [regularizer](../regularizers.md)).\n",
      " |      activity_regularizer: Regularizer function applied to\n",
      " |          the output of the layer (its \"activation\").\n",
      " |          (see [regularizer](../regularizers.md)).\n",
      " |      kernel_constraint: Constraint function applied to\n",
      " |          the `kernel` weights matrix\n",
      " |          (see [constraints](../constraints.md)).\n",
      " |      recurrent_constraint: Constraint function applied to\n",
      " |          the `recurrent_kernel` weights matrix\n",
      " |          (see [constraints](../constraints.md)).\n",
      " |      bias_constraint: Constraint function applied to the bias vector\n",
      " |          (see [constraints](../constraints.md)).\n",
      " |      dropout: Float between 0 and 1.\n",
      " |          Fraction of the units to drop for\n",
      " |          the linear transformation of the inputs.\n",
      " |      recurrent_dropout: Float between 0 and 1.\n",
      " |          Fraction of the units to drop for\n",
      " |          the linear transformation of the recurrent state.\n",
      " |      implementation: Implementation mode, either 1 or 2.\n",
      " |          Mode 1 will structure its operations as a larger number of\n",
      " |          smaller dot products and additions, whereas mode 2 will\n",
      " |          batch them into fewer, larger operations. These modes will\n",
      " |          have different performance profiles on different hardware and\n",
      " |          for different applications.\n",
      " |      return_sequences: Boolean. Whether to return the last output\n",
      " |          in the output sequence, or the full sequence.\n",
      " |      return_state: Boolean. Whether to return the last state\n",
      " |          in addition to the output.\n",
      " |      go_backwards: Boolean (default False).\n",
      " |          If True, process the input sequence backwards and return the\n",
      " |          reversed sequence.\n",
      " |      stateful: Boolean (default False). If True, the last state\n",
      " |          for each sample at index i in a batch will be used as initial\n",
      " |          state for the sample of index i in the following batch.\n",
      " |      unroll: Boolean (default False).\n",
      " |          If True, the network will be unrolled,\n",
      " |          else a symbolic loop will be used.\n",
      " |          Unrolling can speed-up a RNN,\n",
      " |          although it tends to be more memory-intensive.\n",
      " |          Unrolling is only suitable for short sequences.\n",
      " |  \n",
      " |  # References\n",
      " |      - [Long short-term memory]\n",
      " |        (http://www.bioinf.jku.at/publications/older/2604.pdf)\n",
      " |      - [Learning to forget: Continual prediction with LSTM]\n",
      " |        (http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n",
      " |      - [Supervised sequence labeling with recurrent neural networks]\n",
      " |        (http://www.cs.toronto.edu/~graves/preprint.pdf)\n",
      " |      - [A Theoretically Grounded Application of Dropout in\n",
      " |         Recurrent Neural Networks](https://arxiv.org/abs/1512.05287)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LSTM\n",
      " |      RNN\n",
      " |      keras.engine.base_layer.Layer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  call(self, inputs, mask=None, training=None, initial_state=None)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: Input tensor, or list/tuple of input tensors.\n",
      " |          **kwargs: Additional keyword arguments.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      # Returns\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      # Arguments\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  activation\n",
      " |  \n",
      " |  bias_constraint\n",
      " |  \n",
      " |  bias_initializer\n",
      " |  \n",
      " |  bias_regularizer\n",
      " |  \n",
      " |  dropout\n",
      " |  \n",
      " |  implementation\n",
      " |  \n",
      " |  kernel_constraint\n",
      " |  \n",
      " |  kernel_initializer\n",
      " |  \n",
      " |  kernel_regularizer\n",
      " |  \n",
      " |  recurrent_activation\n",
      " |  \n",
      " |  recurrent_constraint\n",
      " |  \n",
      " |  recurrent_dropout\n",
      " |  \n",
      " |  recurrent_initializer\n",
      " |  \n",
      " |  recurrent_regularizer\n",
      " |  \n",
      " |  unit_forget_bias\n",
      " |  \n",
      " |  units\n",
      " |  \n",
      " |  use_bias\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from RNN:\n",
      " |  \n",
      " |  __call__(self, inputs, initial_state=None, constants=None, **kwargs)\n",
      " |      Wrapper around self.call(), for handling internal references.\n",
      " |      \n",
      " |      If a Keras tensor is passed:\n",
      " |          - We call self._add_inbound_node().\n",
      " |          - If necessary, we `build` the layer to match\n",
      " |              the _keras_shape of the input(s).\n",
      " |          - We update the _keras_shape of every input tensor with\n",
      " |              its new shape (obtained via self.compute_output_shape).\n",
      " |              This is done as part of _add_inbound_node().\n",
      " |          - We update the _keras_history of the output tensor(s)\n",
      " |              with the current layer.\n",
      " |              This is done as part of _add_inbound_node().\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: Can be a tensor or list/tuple of tensors.\n",
      " |          **kwargs: Additional keyword arguments to be passed to `call()`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output of the layer's `call` method.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: in case the layer is missing shape information\n",
      " |              for its `build` call.\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the layer weights.\n",
      " |      \n",
      " |      Must be implemented on all layers that have weights.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          input_shape: Keras tensor (future input to layer)\n",
      " |              or list/tuple of Keras tensors to reference\n",
      " |              for weight shape computations.\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      # Returns\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      Assumes that the layer will be built\n",
      " |      to match that input shape provided.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  get_initial_state(self, inputs)\n",
      " |  \n",
      " |  get_losses_for(self, inputs=None)\n",
      " |  \n",
      " |  reset_states(self, states=None)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from RNN:\n",
      " |  \n",
      " |  losses\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |  \n",
      " |  states\n",
      " |  \n",
      " |  trainable_weights\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  add_loss(self, losses, inputs=None)\n",
      " |      Adds losses to the layer.\n",
      " |      \n",
      " |      The loss may potentially be conditional on some inputs tensors,\n",
      " |      for instance activity losses are conditional on the layer's inputs.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          losses: loss tensor or list of loss tensors\n",
      " |              to add to the layer.\n",
      " |          inputs: input tensor or list of inputs tensors to mark\n",
      " |              the losses as conditional on these inputs.\n",
      " |              If None is passed, the loss is assumed unconditional\n",
      " |              (e.g. L2 weight regularization, which only depends\n",
      " |              on the layer's weights variables, not on any inputs tensors).\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Adds updates to the layer.\n",
      " |      \n",
      " |      The updates may potentially be conditional on some inputs tensors,\n",
      " |      for instance batch norm updates are conditional on the layer's inputs.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          updates: update op or list of update ops\n",
      " |              to add to the layer.\n",
      " |          inputs: input tensor or list of inputs tensors to mark\n",
      " |              the updates as conditional on these inputs.\n",
      " |              If None is passed, the updates are assumed unconditional.\n",
      " |  \n",
      " |  add_weight(self, name, shape, dtype=None, initializer=None, regularizer=None, trainable=True, constraint=None)\n",
      " |      Adds a weight variable to the layer.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          name: String, the name for the weight variable.\n",
      " |          shape: The shape tuple of the weight.\n",
      " |          dtype: The dtype of the weight.\n",
      " |          initializer: An Initializer instance (callable).\n",
      " |          regularizer: An optional Regularizer instance.\n",
      " |          trainable: A boolean, whether the weight should\n",
      " |              be trained via backprop or not (assuming\n",
      " |              that the layer itself is also trainable).\n",
      " |          constraint: An optional Constraint instance.\n",
      " |      \n",
      " |      # Returns\n",
      " |          The created weight variable.\n",
      " |  \n",
      " |  assert_input_compatibility(self, inputs)\n",
      " |      Checks compatibility between the layer and provided inputs.\n",
      " |      \n",
      " |      This checks that the tensor(s) `input`\n",
      " |      verify the input assumptions of the layer\n",
      " |      (if any). If not, exceptions are raised.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: input tensor or list of input tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: in case of mismatch between\n",
      " |              the provided inputs and the expectations of the layer.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Counts the total number of scalars composing the weights.\n",
      " |      \n",
      " |      # Returns\n",
      " |          An integer count.\n",
      " |      \n",
      " |      # Raises\n",
      " |          RuntimeError: if the layer isn't yet built\n",
      " |              (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Weights values as a list of numpy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from Numpy arrays.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          weights: a list of Numpy arrays. The number\n",
      " |              of arrays and their shape must match\n",
      " |              number of the dimensions of the weights\n",
      " |              of the layer (i.e. it should match the\n",
      " |              output of `get_weights`).\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: If the provided weights list does not match the\n",
      " |              layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  built\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape tuple(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input shape tuple\n",
      " |          (or list of input shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output tensor or list of output tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape tuple(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one inbound node,\n",
      " |      or if all inbound nodes have the same output shape.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output shape tuple\n",
      " |          (or list of input shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  weights\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "help(LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 13, 50)       56900       input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 15, 50)       99500       input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   [(None, 50), (None,  20200       embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 15, 50)       20200       embedding_6[0][0]                \n",
      "                                                                 lstm_5[0][1]                     \n",
      "                                                                 lstm_5[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 15, 1990)     101490      lstm_6[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 298,290\n",
      "Trainable params: 298,290\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 810 samples, validate on 90 samples\n",
      "Epoch 1/30\n",
      "810/810 [==============================] - 10s 13ms/step - loss: 1.6410 - acc: 0.0738 - val_loss: 4.5402 - val_acc: 0.0748\n",
      "Epoch 2/30\n",
      "810/810 [==============================] - 10s 13ms/step - loss: 1.6181 - acc: 0.0743 - val_loss: 4.4469 - val_acc: 0.0770\n",
      "Epoch 3/30\n",
      "810/810 [==============================] - 10s 12ms/step - loss: 1.5980 - acc: 0.0753 - val_loss: 4.6773 - val_acc: 0.0778\n",
      "Epoch 4/30\n",
      "810/810 [==============================] - 10s 13ms/step - loss: 1.5787 - acc: 0.0766 - val_loss: 4.6074 - val_acc: 0.0785\n",
      "Epoch 5/30\n",
      "810/810 [==============================] - 10s 13ms/step - loss: 1.5600 - acc: 0.0770 - val_loss: 4.5289 - val_acc: 0.0778\n",
      "Epoch 6/30\n",
      "810/810 [==============================] - 11s 13ms/step - loss: 1.5432 - acc: 0.0776 - val_loss: 4.4371 - val_acc: 0.0793\n",
      "Epoch 7/30\n",
      "810/810 [==============================] - 10s 13ms/step - loss: 1.5295 - acc: 0.0791 - val_loss: 4.5574 - val_acc: 0.0785\n",
      "Epoch 8/30\n",
      "810/810 [==============================] - 10s 13ms/step - loss: 1.5141 - acc: 0.0791 - val_loss: 4.5773 - val_acc: 0.0785\n",
      "Epoch 9/30\n",
      "810/810 [==============================] - 10s 13ms/step - loss: 1.4996 - acc: 0.0796 - val_loss: 4.4815 - val_acc: 0.0793\n",
      "Epoch 10/30\n",
      "810/810 [==============================] - 10s 13ms/step - loss: 1.4867 - acc: 0.0797 - val_loss: 4.5416 - val_acc: 0.0785\n",
      "Epoch 11/30\n",
      "810/810 [==============================] - 10s 13ms/step - loss: 1.4755 - acc: 0.0803 - val_loss: 4.6330 - val_acc: 0.0778\n",
      "Epoch 12/30\n",
      "810/810 [==============================] - 10s 12ms/step - loss: 1.4633 - acc: 0.0807 - val_loss: 4.5968 - val_acc: 0.0778\n",
      "Epoch 13/30\n",
      "810/810 [==============================] - 10s 13ms/step - loss: 1.4511 - acc: 0.0814 - val_loss: 4.6128 - val_acc: 0.0756\n",
      "Epoch 14/30\n",
      "810/810 [==============================] - 10s 13ms/step - loss: 1.4408 - acc: 0.0822 - val_loss: 4.6072 - val_acc: 0.0748\n",
      "Epoch 15/30\n",
      "810/810 [==============================] - 10s 13ms/step - loss: 1.4294 - acc: 0.0819 - val_loss: 4.6150 - val_acc: 0.0756\n",
      "Epoch 16/30\n",
      "810/810 [==============================] - 10s 13ms/step - loss: 1.4185 - acc: 0.0828 - val_loss: 4.5969 - val_acc: 0.0689\n",
      "Epoch 17/30\n",
      "810/810 [==============================] - 10s 13ms/step - loss: 1.4066 - acc: 0.0832 - val_loss: 4.5608 - val_acc: 0.0659\n",
      "Epoch 18/30\n",
      "810/810 [==============================] - 10s 13ms/step - loss: 1.3962 - acc: 0.0844 - val_loss: 4.5728 - val_acc: 0.0607\n",
      "Epoch 19/30\n",
      "810/810 [==============================] - 10s 13ms/step - loss: 1.3856 - acc: 0.0846 - val_loss: 4.5883 - val_acc: 0.0600\n",
      "Epoch 20/30\n",
      "810/810 [==============================] - 10s 13ms/step - loss: 1.3762 - acc: 0.0853 - val_loss: 4.5861 - val_acc: 0.0578\n",
      "Epoch 21/30\n",
      "810/810 [==============================] - 10s 13ms/step - loss: 1.3653 - acc: 0.0861 - val_loss: 4.6081 - val_acc: 0.0533\n",
      "Epoch 22/30\n",
      "810/810 [==============================] - 10s 13ms/step - loss: 1.3551 - acc: 0.0867 - val_loss: 4.6258 - val_acc: 0.0548\n",
      "Epoch 23/30\n",
      "810/810 [==============================] - 10s 13ms/step - loss: 1.3456 - acc: 0.0865 - val_loss: 4.5172 - val_acc: 0.0526\n",
      "Epoch 24/30\n",
      "810/810 [==============================] - 10s 13ms/step - loss: 1.3361 - acc: 0.0887 - val_loss: 4.5667 - val_acc: 0.0526\n",
      "Epoch 25/30\n",
      "810/810 [==============================] - 10s 13ms/step - loss: 1.3273 - acc: 0.0889 - val_loss: 4.6145 - val_acc: 0.0548\n",
      "Epoch 26/30\n",
      "810/810 [==============================] - 10s 13ms/step - loss: 1.3181 - acc: 0.0900 - val_loss: 4.6841 - val_acc: 0.0511\n",
      "Epoch 27/30\n",
      "810/810 [==============================] - 10s 13ms/step - loss: 1.3102 - acc: 0.0894 - val_loss: 4.5933 - val_acc: 0.0526\n",
      "Epoch 28/30\n",
      "810/810 [==============================] - 10s 13ms/step - loss: 1.2995 - acc: 0.0909 - val_loss: 4.6298 - val_acc: 0.0519\n",
      "Epoch 29/30\n",
      "810/810 [==============================] - 11s 14ms/step - loss: 1.2912 - acc: 0.0917 - val_loss: 4.6144 - val_acc: 0.0541\n",
      "Epoch 30/30\n",
      "810/810 [==============================] - 10s 13ms/step - loss: 1.2836 - acc: 0.0920 - val_loss: 4.6153 - val_acc: 0.0504\n"
     ]
    }
   ],
   "source": [
    "h = model.fit([encoder_input_data, decoder_input_data], decoder_output_data,\n",
    "              batch_size=16, epochs=30, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1db89226eb8>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(h.history['loss'], 'b-')\n",
    "plt.plot(h.history['val_loss'], 'r-')\n",
    "#plt.ylim(0,3)\n",
    "plt.legend(['loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 번역하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love you'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['i', 'love', 'you']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'그 약은 두렵지 것이다 _END'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = 'I love you'\n",
    "l = s.lower().split()\n",
    "l = [word for word in l if word in eng_words][:eng_sen_max]\n",
    "\n",
    "### 영어 입력\n",
    "eng_text = np.zeros([1,eng_sen_max])\n",
    "for t,word in enumerate(l):\n",
    "    eng_text[0,t] = eng_w2i[word]\n",
    "\n",
    "### 한국어 입력\n",
    "kor_text = np.zeros([1,kor_sen_max])\n",
    "kor_text[0,0] = kor_w2i['START_']\n",
    "result = []\n",
    "\n",
    "for i in range(kor_sen_max):\n",
    "    pred_y = model.predict([eng_text,kor_text])[0,i]\n",
    "    \n",
    "    idx = np.argmax(pred_y)\n",
    "    word = kor_i2w[idx]\n",
    "    result.append(word)\n",
    "    \n",
    "    if word=='_END' or i==(kor_sen_max-1): break\n",
    "        \n",
    "    kor_text[0,i+1] = idx\n",
    "    \n",
    "display(s,l,' '.join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
