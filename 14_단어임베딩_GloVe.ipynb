{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 빅데이터 활용 AI 설계\n",
    "# GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe 적용\n",
    "- glove 는 스탠포드 대학교에서 만든 2014년 영문 위키피디아를 사용하여 사전에 계산한 임베딩임\n",
    "- 구글의 Word2vec 와 함께 인기가 많다\n",
    "- https://nlp.stanford.edu/projects/glove\n",
    "- https://bit.ly/2NIJwdb 에서 glove.6B.zip 다운로드 (이 중 glove.6B.100d.txt 사용)\n",
    "- 단어 40만개, 100차원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe 단어사전 둘러보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../../프로그래밍/텍스트마이닝/glove.6B.100d.txt', encoding='utf8')\n",
    "s = f.readline()\n",
    "s2 = f.readline()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s.strip().split()) # 단어와 100 차원의 벡터 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 0\n",
    "\n",
    "f = open('../../프로그래밍/텍스트마이닝/glove.6B.100d.txt', encoding='utf8')\n",
    "for line in f:\n",
    "    n += 1\n",
    "f.close()\n",
    "\n",
    "n # 40만 개의 단어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe 단어사전 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = {}\n",
    "\n",
    "f = open('../../프로그래밍/텍스트마이닝/glove.6B.100d.txt', encoding='utf8')\n",
    "\n",
    "for line in f:\n",
    "    l = line.strip().split()\n",
    "    for i in range(1,101): l[i] = float(l[i])\n",
    "    glove[l[0]] = l[1:]\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove), len(glove['apple'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imdb 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\py36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000,), (25000,), (25000,), (25000,))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0]) # 단어들의 인덱스임 (0~9999 인 단어만 있음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단어사전 얻기(단어, 인덱스)\n",
    "- C:\\Users\\사용자아이디\\.keras\\datasets\\imdb_word_index.json\n",
    "- X_train 의 인덱스 값에서 3을 빼줘야 word_index 의 인덱스 값에 해당함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dic = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fawn 34701\n",
      "tsukino 52006\n",
      "nunnery 52007\n",
      "sonja 16816\n",
      "vani 63951\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for k in imdb_dic:\n",
    "    print(k, imdb_dic[k])\n",
    "    n += 1\n",
    "    if n>=5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras imdb 데이터 주의사항\n",
    "- https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset 참고\n",
    "- 0 번과 3번이 빠져있음\n",
    "- 0: PAD, 1: START, 2: UNKNOWN, 3: ???, 4번 부터 실제 단어임\n",
    "- imdb.get_word_index() 의 단어사전은 3 만큼의 차이가 생긴다. 그리고 0번 단어가 없다.\n",
    "- X_train 에서의 4 는 단어사전에서는 4-3, 즉 1 이 된다. (index_form 옵션의 기본값이 3이다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "for l in X_train:\n",
    "    result += l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = np.bincount(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([     0,  25000, 341611,      0, 336148, 164097, 163040, 145847,\n",
       "        135708, 107313], dtype=int64), 10000)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc[:10], len(bc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3], dtype=int64)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(bc==0)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한 문장을 20개의 단어로 만들기\n",
    "- 0,1,2,3 은 제외하고 20개 단어로 만든다. (실제로 0과 3은 하나도 없다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "\n",
    "for s in X_train:\n",
    "    l = []\n",
    "    for i in s:\n",
    "        if i>3: l += [i]\n",
    "            \n",
    "    l = l[:20]\n",
    "    if len(l)<20: l += [0]*(20-len(l))\n",
    "        \n",
    "    X.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 20)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array(X)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### glove 와 imdb 단어집 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_i_to_w = {v:k for k,v in imdb_dic.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_i_to_w[1] # 0 은 없다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_vector(n):\n",
    "    if n<4: return None\n",
    "    \n",
    "    word = imdb_i_to_w[n-3]\n",
    "    return glove.get(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.zeros([25000,20,100])\n",
    "\n",
    "for r in range(25000):\n",
    "    for c in range(20):\n",
    "        v = get_glove_vector(X_train[r,c])\n",
    "        if v!=None: X[r,c] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 20, 100)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 2000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               512256    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 512,513\n",
      "Trainable params: 512,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(20,100)))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 30s 2ms/step - loss: 0.6794 - acc: 0.5951 - val_loss: 0.6411 - val_acc: 0.6332\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 27s 1ms/step - loss: 0.6102 - acc: 0.6592 - val_loss: 0.6677 - val_acc: 0.6084\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.5446 - acc: 0.7154 - val_loss: 0.7191 - val_acc: 0.6162\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.4506 - acc: 0.7839 - val_loss: 0.6967 - val_acc: 0.6384\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.3314 - acc: 0.8544 - val_loss: 0.8250 - val_acc: 0.6190\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.2266 - acc: 0.9083 - val_loss: 0.9517 - val_acc: 0.6218\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.1457 - acc: 0.9456 - val_loss: 1.2002 - val_acc: 0.6096\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.0925 - acc: 0.9669 - val_loss: 1.2274 - val_acc: 0.6230\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.0641 - acc: 0.9782 - val_loss: 1.3373 - val_acc: 0.6166\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.0438 - acc: 0.9873 - val_loss: 1.5785 - val_acc: 0.6164\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27af2fdbcc0>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "model.fit(X, y_train, batch_size=32, epochs=10, validation_split=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
